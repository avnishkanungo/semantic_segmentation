{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2dfc01-76b4-4f60-aba0-e5d8c3394ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision                  \n",
    "import torchvision.transforms as tvt\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import ImageFilter\n",
    "import numbers\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import pickle\n",
    "import pymsgbox\n",
    "import time\n",
    "import logging\n",
    "sys.path.append( \"/Users/avnishkanungo/Desktop/Purdue /ECE60146/DLStudio/DLStudio-2.3.6/DLStudio\" )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seed = 448         \n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "numpy.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmarks=False\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "\n",
    "##  watch -d -n 0.5 nvidia-smi\n",
    "\n",
    "from DLStudio import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361ab7e-de17-497f-abbc-758f5b8efcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code has been taken from Professor Kak' DL Studio Model.\n",
    "\n",
    "class SemanticSegmentation(nn.Module):             \n",
    "        \n",
    "            def __init__(self, dl_studio, max_num_objects, dataserver_train=None, dataserver_test=None, dataset_file_train=None, dataset_file_test=None):\n",
    "                super(SemanticSegmentation, self).__init__()\n",
    "                self.dl_studio = dl_studio\n",
    "                self.max_num_objects = max_num_objects\n",
    "                self.dataserver_train = dataserver_train\n",
    "                self.dataserver_test = dataserver_test\n",
    "    \n",
    "            class PurdueShapes5MultiObjectDataset(torch.utils.data.Dataset):\n",
    "               \n",
    "                def __init__(self, dl_studio, segmenter, train_or_test, dataset_file):\n",
    "                    super(SemanticSegmentation.PurdueShapes5MultiObjectDataset, self).__init__()\n",
    "                    max_num_objects = segmenter.max_num_objects\n",
    "                    if train_or_test == 'train' and dataset_file == \"PurdueShapes5MultiObject-10000-train.gz\":\n",
    "                        if os.path.exists(\"torch_saved_PurdueShapes5MultiObject-10000_dataset.pt\") and \\\n",
    "                                  os.path.exists(\"torch_saved_PurdueShapes5MultiObject_label_map.pt\"):\n",
    "                            print(\"\\nLoading training data from torch saved file\")\n",
    "                            self.dataset = torch.load(\"torch_saved_PurdueShapes5MultiObject-10000_dataset.pt\")\n",
    "                            self.label_map = torch.load(\"torch_saved_PurdueShapes5MultiObject_label_map.pt\")\n",
    "                            self.num_shapes = len(self.label_map)\n",
    "                            self.image_size = dl_studio.image_size\n",
    "                        else: \n",
    "                            print(\"\"\"\\n\\n\\nLooks like this is the first time you will be loading in\\n\"\"\"\n",
    "                                  \"\"\"the dataset for this script. First time loading could take\\n\"\"\"\n",
    "                                  \"\"\"a few minutes.  Any subsequent attempts will only take\\n\"\"\"\n",
    "                                  \"\"\"a few seconds.\\n\\n\\n\"\"\")\n",
    "                            root_dir = dl_studio.dataroot\n",
    "                            f = gzip.open(root_dir + dataset_file, 'rb')\n",
    "                            dataset = f.read()\n",
    "                            self.dataset, self.label_map = pickle.loads(dataset, encoding='latin1')\n",
    "                            torch.save(self.dataset, \"torch_saved_PurdueShapes5MultiObject-10000_dataset.pt\")\n",
    "                            torch.save(self.label_map, \"torch_saved_PurdueShapes5MultiObject_label_map.pt\")\n",
    "                            # reverse the key-value pairs in the label dictionary:\n",
    "                            self.class_labels = dict(map(reversed, self.label_map.items()))\n",
    "                            self.num_shapes = len(self.class_labels)\n",
    "                            self.image_size = dl_studio.image_size\n",
    "                    else:\n",
    "                        root_dir = dl_studio.dataroot\n",
    "                        f = gzip.open(root_dir + dataset_file, 'rb')\n",
    "                        dataset = f.read()\n",
    "                        if sys.version_info[0] == 3:\n",
    "                            self.dataset, self.label_map = pickle.loads(dataset, encoding='latin1')\n",
    "                        else:\n",
    "                            self.dataset, self.label_map = pickle.loads(dataset)\n",
    "                        # reverse the key-value pairs in the label dictionary:\n",
    "                        self.class_labels = dict(map(reversed, self.label_map.items()))\n",
    "                        self.num_shapes = len(self.class_labels)\n",
    "                        self.image_size = dl_studio.image_size\n",
    "    \n",
    "                def __len__(self):\n",
    "                    return len(self.dataset)\n",
    "    \n",
    "                def __getitem__(self, idx):\n",
    "                    image_size = self.image_size\n",
    "                    r = np.array( self.dataset[idx][0] )\n",
    "                    g = np.array( self.dataset[idx][1] )\n",
    "                    b = np.array( self.dataset[idx][2] )\n",
    "                    R,G,B = r.reshape(image_size[0],image_size[1]), g.reshape(image_size[0],image_size[1]), b.reshape(image_size[0],image_size[1])\n",
    "                    im_tensor = torch.zeros(3,image_size[0],image_size[1], dtype=torch.float)\n",
    "                    im_tensor[0,:,:] = torch.from_numpy(R)\n",
    "                    im_tensor[1,:,:] = torch.from_numpy(G)\n",
    "                    im_tensor[2,:,:] = torch.from_numpy(B)\n",
    "                    mask_array = np.array(self.dataset[idx][3])\n",
    "                    max_num_objects = len( mask_array[0] ) \n",
    "                    mask_tensor = torch.from_numpy(mask_array)\n",
    "                    mask_val_to_bbox_map =  self.dataset[idx][4]\n",
    "                    max_bboxes_per_entry_in_map = max([ len(mask_val_to_bbox_map[key]) for key in mask_val_to_bbox_map ])\n",
    "                    ##  The first arg 5 is for the number of bboxes we are going to need. If all the\n",
    "                    ##  shapes are exactly the same, you are going to need five different bbox'es.\n",
    "                    ##  The second arg is the index reserved for each shape in a single bbox\n",
    "                    bbox_tensor = torch.zeros(max_num_objects,self.num_shapes,4, dtype=torch.float)\n",
    "                    for bbox_idx in range(max_bboxes_per_entry_in_map):\n",
    "                        for key in mask_val_to_bbox_map:\n",
    "                            if len(mask_val_to_bbox_map[key]) == 1:\n",
    "                                if bbox_idx == 0:\n",
    "                                    bbox_tensor[bbox_idx,key,:] = torch.from_numpy(np.array(mask_val_to_bbox_map[key][bbox_idx]))\n",
    "                            elif len(mask_val_to_bbox_map[key]) > 1 and bbox_idx < len(mask_val_to_bbox_map[key]):\n",
    "                                bbox_tensor[bbox_idx,key,:] = torch.from_numpy(np.array(mask_val_to_bbox_map[key][bbox_idx]))\n",
    "                    sample = {'image'        : im_tensor, \n",
    "                              'mask_tensor'  : mask_tensor,\n",
    "                              'bbox_tensor'  : bbox_tensor }\n",
    "                    return sample\n",
    "    \n",
    "            def load_PurdueShapes5MultiObject_dataset(self, dataserver_train, dataserver_test ):   \n",
    "                self.train_dataloader = torch.utils.data.DataLoader(dataserver_train,\n",
    "                            batch_size=self.dl_studio.batch_size,shuffle=True)\n",
    "                self.test_dataloader = torch.utils.data.DataLoader(dataserver_test,\n",
    "                                   batch_size=self.dl_studio.batch_size,shuffle=False)\n",
    "    \n",
    "    \n",
    "            class SkipBlockDN(nn.Module):\n",
    "                \n",
    "                def __init__(self, in_ch, out_ch, downsample=False, skip_connections=True):\n",
    "                    super(SemanticSegmentation.SkipBlockDN, self).__init__()\n",
    "                    self.downsample = downsample\n",
    "                    self.skip_connections = skip_connections\n",
    "                    self.in_ch = in_ch\n",
    "                    self.out_ch = out_ch\n",
    "                    self.convo1 = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)\n",
    "                    self.convo2 = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)\n",
    "                    self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "                    self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "                    if downsample:\n",
    "                        self.downsampler = nn.Conv2d(in_ch, out_ch, 1, stride=2)\n",
    "                def forward(self, x):\n",
    "                    identity = x                                     \n",
    "                    out = self.convo1(x)                              \n",
    "                    out = self.bn1(out)                              \n",
    "                    out = nn.functional.relu(out)\n",
    "                    if self.in_ch == self.out_ch:\n",
    "                        out = self.convo2(out)                              \n",
    "                        out = self.bn2(out)                              \n",
    "                        out = nn.functional.relu(out)\n",
    "                    if self.downsample:\n",
    "                        out = self.downsampler(out)\n",
    "                        identity = self.downsampler(identity)\n",
    "                    if self.skip_connections:\n",
    "                        if self.in_ch == self.out_ch:\n",
    "                            out = out + identity\n",
    "                        else:\n",
    "                            out = out + torch.cat((identity, identity), dim=1) \n",
    "                    return out\n",
    "    \n",
    "    \n",
    "            class SkipBlockUP(nn.Module):\n",
    "                \n",
    "                def __init__(self, in_ch, out_ch, upsample=False, skip_connections=True):\n",
    "                    super(SemanticSegmentation.SkipBlockUP, self).__init__()\n",
    "                    self.upsample = upsample\n",
    "                    self.skip_connections = skip_connections\n",
    "                    self.in_ch = in_ch\n",
    "                    self.out_ch = out_ch\n",
    "                    self.convoT1 = nn.ConvTranspose2d(in_ch, out_ch, 3, padding=1)\n",
    "                    self.convoT2 = nn.ConvTranspose2d(in_ch, out_ch, 3, padding=1)\n",
    "                    self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "                    self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "                    if upsample:\n",
    "                        self.upsampler = nn.ConvTranspose2d(in_ch, out_ch, 1, stride=2, dilation=2, output_padding=1, padding=0)\n",
    "                def forward(self, x):\n",
    "                    identity = x                                     \n",
    "                    out = self.convoT1(x)                              \n",
    "                    out = self.bn1(out)                              \n",
    "                    out = nn.functional.relu(out)\n",
    "                    out  =  nn.ReLU(inplace=False)(out)            \n",
    "                    if self.in_ch == self.out_ch:\n",
    "                        out = self.convoT2(out)                              \n",
    "                        out = self.bn2(out)                              \n",
    "                        out = nn.functional.relu(out)\n",
    "                    if self.upsample:\n",
    "                        out = self.upsampler(out)\n",
    "                        identity = self.upsampler(identity)\n",
    "                    if self.skip_connections:\n",
    "                        if self.in_ch == self.out_ch:\n",
    "                            out = out + identity                              \n",
    "                        else:\n",
    "                            out = out + identity[:,self.out_ch:,:,:]\n",
    "                    return out\n",
    "            \n",
    "    \n",
    "            class mUnet(nn.Module):\n",
    "               \n",
    "                def __init__(self, skip_connections=True, depth=16):\n",
    "                    super(SemanticSegmentation.mUnet, self).__init__()\n",
    "                    self.depth = depth // 2\n",
    "                    self.conv_in = nn.Conv2d(3, 64, 3, padding=1)\n",
    "                    ##  For the DN arm of the U:\n",
    "                    self.bn1DN  = nn.BatchNorm2d(64)\n",
    "                    self.bn2DN  = nn.BatchNorm2d(128)\n",
    "                    self.skip64DN_arr = nn.ModuleList()\n",
    "                    for i in range(self.depth):\n",
    "                        self.skip64DN_arr.append(SemanticSegmentation.SkipBlockDN(64, 64, skip_connections=skip_connections))\n",
    "                    self.skip64dsDN = SemanticSegmentation.SkipBlockDN(64, 64,   downsample=True, skip_connections=skip_connections)\n",
    "                    self.skip64to128DN = SemanticSegmentation.SkipBlockDN(64, 128, skip_connections=skip_connections )\n",
    "                    self.skip128DN_arr = nn.ModuleList()\n",
    "                    for i in range(self.depth):\n",
    "                        self.skip128DN_arr.append(SemanticSegmentation.SkipBlockDN(128, 128, skip_connections=skip_connections))\n",
    "                    self.skip128dsDN = SemanticSegmentation.SkipBlockDN(128,128, downsample=True, skip_connections=skip_connections)\n",
    "                    ##  For the UP arm of the U:\n",
    "                    self.bn1UP  = nn.BatchNorm2d(128)\n",
    "                    self.bn2UP  = nn.BatchNorm2d(64)\n",
    "                    self.skip64UP_arr = nn.ModuleList()\n",
    "                    for i in range(self.depth):\n",
    "                        self.skip64UP_arr.append(SemanticSegmentation.SkipBlockUP(64, 64, skip_connections=skip_connections))\n",
    "                    self.skip64usUP = SemanticSegmentation.SkipBlockUP(64, 64, upsample=True, skip_connections=skip_connections)\n",
    "                    self.skip128to64UP = SemanticSegmentation.SkipBlockUP(128, 64, skip_connections=skip_connections )\n",
    "                    self.skip128UP_arr = nn.ModuleList()\n",
    "                    for i in range(self.depth):\n",
    "                        self.skip128UP_arr.append(SemanticSegmentation.SkipBlockUP(128, 128, skip_connections=skip_connections))\n",
    "                    self.skip128usUP = SemanticSegmentation.SkipBlockUP(128,128, upsample=True, skip_connections=skip_connections)\n",
    "                    self.conv_out = nn.ConvTranspose2d(64, 5, 3, stride=2,dilation=2,output_padding=1,padding=2)\n",
    "    \n",
    "                def forward(self, x):\n",
    "                    ##  Going down to the bottom of the U:\n",
    "                    x = nn.MaxPool2d(2,2)(nn.functional.relu(self.conv_in(x)))          \n",
    "                    for i,skip64 in enumerate(self.skip64DN_arr[:self.depth//4]):\n",
    "                        x = skip64(x)                \n",
    "            \n",
    "                    num_channels_to_save1 = x.shape[1] // 2\n",
    "                    save_for_upside_1 = x[:,:num_channels_to_save1,:,:].clone()\n",
    "                    x = self.skip64dsDN(x)\n",
    "                    for i,skip64 in enumerate(self.skip64DN_arr[self.depth//4:]):\n",
    "                        x = skip64(x)                \n",
    "                    x = self.bn1DN(x)\n",
    "                    num_channels_to_save2 = x.shape[1] // 2\n",
    "                    save_for_upside_2 = x[:,:num_channels_to_save2,:,:].clone()\n",
    "                    x = self.skip64to128DN(x)\n",
    "                    for i,skip128 in enumerate(self.skip128DN_arr[:self.depth//4]):\n",
    "                        x = skip128(x)                \n",
    "            \n",
    "                    x = self.bn2DN(x)\n",
    "                    num_channels_to_save3 = x.shape[1] // 2\n",
    "                    save_for_upside_3 = x[:,:num_channels_to_save3,:,:].clone()\n",
    "                    for i,skip128 in enumerate(self.skip128DN_arr[self.depth//4:]):\n",
    "                        x = skip128(x)                \n",
    "                    x = self.skip128dsDN(x)\n",
    "                    ## Coming up from the bottom of U on the other side:\n",
    "                    x = self.skip128usUP(x)          \n",
    "                    for i,skip128 in enumerate(self.skip128UP_arr[:self.depth//4]):\n",
    "                        x = skip128(x)                \n",
    "                    x[:,:num_channels_to_save3,:,:] =  save_for_upside_3\n",
    "                    x = self.bn1UP(x)\n",
    "                    for i,skip128 in enumerate(self.skip128UP_arr[:self.depth//4]):\n",
    "                        x = skip128(x)                \n",
    "                    x = self.skip128to64UP(x)\n",
    "                    for i,skip64 in enumerate(self.skip64UP_arr[self.depth//4:]):\n",
    "                        x = skip64(x)                \n",
    "                    x[:,:num_channels_to_save2,:,:] =  save_for_upside_2\n",
    "                    x = self.bn2UP(x)\n",
    "                    x = self.skip64usUP(x)\n",
    "                    for i,skip64 in enumerate(self.skip64UP_arr[:self.depth//4]):\n",
    "                        x = skip64(x)                \n",
    "                    x[:,:num_channels_to_save1,:,:] =  save_for_upside_1\n",
    "                    x = self.conv_out(x)\n",
    "                    return x\n",
    "            \n",
    "\n",
    "######################### Combined Dice Loss Implemenation #####################################################################################################\n",
    "    \n",
    "            class MSEPlusDiceLoss(nn.Module):\n",
    "                def __init__(self, batch_size, alpha=0.5, epsilon= 1e-3): # batch size, scale factor and epsilon(to ensure denominator does not equal zero)\n",
    "                    super(SemanticSegmentation.MSEPlusDiceLoss, self).__init__()\n",
    "                    self.batch_size = batch_size\n",
    "                    self.alpha = alpha\n",
    "                    self.epsilon = epsilon\n",
    "\n",
    "                def forward(self, output, mask_tensor):\n",
    "                    composite_loss = torch.zeros(1, self.batch_size, requires_grad=True) #Tensor defination for calculate the combined loss\n",
    "                    dice_loss = torch.zeros(1, self.batch_size, requires_grad=True)\n",
    "                    mse_loss = torch.zeros(1, self.batch_size, requires_grad=True)\n",
    "                    \n",
    "        \n",
    "                    for idx in range(self.batch_size):\n",
    "                        for mask_layer_idx in range(mask_tensor.shape[0]):\n",
    "                            mask = mask_tensor[idx, mask_layer_idx, :, :]\n",
    "                            output_mask = output[idx, mask_layer_idx, :, :]\n",
    "                            if torch.sum(mask) + torch.sum(output_mask) > 0: #Dice Loss logic implementaiton\n",
    "                                intersection = torch.sum(output[idx, mask_layer_idx, :, :] * mask)\n",
    "                                dice_coefficient = (2. * intersection + self.epsilon ) / (torch.sum(output[idx, mask_layer_idx, :, :]) + torch.sum(mask) + self.epsilon)\n",
    "                                dice_loss_copy = dice_loss.clone()\n",
    "                                dice_loss_copy[0, mask_layer_idx] = 1 - dice_coefficient\n",
    "                                dice_loss = dice_loss_copy\n",
    "                                \n",
    "                                \n",
    "        \n",
    "                        if torch.sum(mask_tensor[idx])>0 and torch.sum(output[idx])>0  :   #MSE Implementation\n",
    "                            mse = nn.MSELoss()\n",
    "                            mse_loss_copy = mse_loss.clone()\n",
    "                            mse_loss_copy[0, idx] = mse(output[idx], mask_tensor[idx])\n",
    "                            mse_loss = mse_loss_copy\n",
    "                        \n",
    "                        \n",
    "                        if torch.sum(mse_loss)>0 and torch.sum(dice_loss)>0: #Combining MSE and Dice Loss\n",
    "                            composite_loss_copy = composite_loss.clone()\n",
    "                            composite_loss_copy[0, idx]= self.alpha*torch.sum(mse_loss) + (1-self.alpha)*torch.sum(dice_loss)\n",
    "                            composite_loss = composite_loss_copy\n",
    "\n",
    "            \n",
    "                    return  torch.sum(composite_loss) / self.batch_size  #, torch.sum(mse_loss), torch.sum(dice_loss)\n",
    "\n",
    "################################################################################################################################################\n",
    "    \n",
    "    \n",
    "            def run_code_for_training_for_semantic_segmentation_dl(self, net):        \n",
    "                filename_for_out1 = \"performance_numbers_\" + str(self.dl_studio.epochs) + \".txt\"\n",
    "                FILE1 = open(filename_for_out1, 'w')\n",
    "                net = copy.deepcopy(net)\n",
    "                net = net.to(self.dl_studio.device)\n",
    "                criterion1 = elf.MSEPlusDiceLoss(self.dl_studio.batch_size, alpha=0.5, epsilon = 1e-3) \n",
    "                optimizer = optim.SGD(net.parameters(), \n",
    "                             lr=self.dl_studio.learning_rate, momentum=self.dl_studio.momentum)\n",
    "                start_time = time.perf_counter()\n",
    "                running_loss = []\n",
    "                for epoch in range(self.dl_studio.epochs):  \n",
    "                    print(\"\")\n",
    "                    running_loss_segmentation = 0.0\n",
    "                    for i, data in enumerate(self.train_dataloader):    \n",
    "                        im_tensor,mask_tensor,bbox_tensor =data['image'],data['mask_tensor'],data['bbox_tensor']\n",
    "                        im_tensor   = im_tensor.to(self.dl_studio.device)\n",
    "                        mask_tensor = mask_tensor.type(torch.FloatTensor)\n",
    "                        mask_tensor = mask_tensor.to(self.dl_studio.device)                 \n",
    "                        bbox_tensor = bbox_tensor.to(self.dl_studio.device)\n",
    "                        optimizer.zero_grad()\n",
    "                        output = net(im_tensor) \n",
    "                        segmentation_loss = criterion1(output, mask_tensor)\n",
    "                        segmentation_loss.requires_grad\n",
    "                        output.requires_grad\n",
    "                        im_tensor.requires_grad = True\n",
    "                        mask_tensor.requires_grad = True \n",
    "                        bbox_tensor.requires_grad = True\n",
    "                        segmentation_loss.backward()\n",
    "                        optimizer.step()\n",
    "                        running_loss_segmentation += segmentation_loss.item()    \n",
    "                        if i%100==99:    \n",
    "                            current_time = time.perf_counter()\n",
    "                            elapsed_time = current_time - start_time\n",
    "                            avg_loss_segmentation = running_loss_segmentation / float(100)\n",
    "                            print(\"[epoch=%d/%d, iter=%4d  elapsed_time=%3d secs]   Combined loss: %f \" % (epoch+1, self.dl_studio.epochs, i+1, elapsed_time, avg_loss_segmentation ))\n",
    "                            running_loss.append(avg_loss_segmentation)\n",
    "                            FILE1.write(\"%.3f\\n\" % avg_loss_segmentation)\n",
    "                            FILE1.flush()\n",
    "                            running_loss_segmentation = 0.0\n",
    "                print(\"\\nFinished Training\\n\")\n",
    "                return running_loss\n",
    "                self.save_model(net)\n",
    "    \n",
    "    \n",
    "            def save_model(self, model):\n",
    "                '''\n",
    "                Save the trained model to a disk file\n",
    "                '''\n",
    "                torch.save(model.state_dict(), self.dl_studio.path_saved_model)\n",
    "    \n",
    "    \n",
    "            def run_code_for_testing_semantic_segmentation_dl(self, net):\n",
    "                net.load_state_dict(torch.load(self.dl_studio.path_saved_model))\n",
    "                batch_size = self.dl_studio.batch_size\n",
    "                image_size = self.dl_studio.image_size\n",
    "                max_num_objects = self.max_num_objects\n",
    "                with torch.no_grad():\n",
    "                    for i, data in enumerate(self.test_dataloader):\n",
    "                        im_tensor,mask_tensor,bbox_tensor =data['image'],data['mask_tensor'],data['bbox_tensor']\n",
    "                        if i % 50 == 0:\n",
    "                            print(\"\\n\\n\\n\\nShowing output for test batch %d: \" % (i+1))\n",
    "                            outputs = net(im_tensor)                        \n",
    "                            ## In the statement below: 1st arg for batch items, 2nd for channels, 3rd and 4th for image size\n",
    "                            output_bw_tensor = torch.zeros(batch_size,1,image_size[0],image_size[1], dtype=float)\n",
    "                            for image_idx in range(batch_size):\n",
    "                                for layer_idx in range(max_num_objects): \n",
    "                                    for m in range(image_size[0]):\n",
    "                                        for n in range(image_size[1]):\n",
    "                                            output_bw_tensor[image_idx,0,m,n]  =  torch.max( outputs[image_idx,:,m,n] )\n",
    "                            display_tensor = torch.zeros(7 * batch_size,3,image_size[0],image_size[1], dtype=float)\n",
    "                            for idx in range(batch_size):\n",
    "                                for bbox_idx in range(max_num_objects):   \n",
    "                                    bb_tensor = bbox_tensor[idx,bbox_idx]\n",
    "                                    for k in range(max_num_objects):\n",
    "                                        i1 = int(bb_tensor[k][1])\n",
    "                                        i2 = int(bb_tensor[k][3])\n",
    "                                        j1 = int(bb_tensor[k][0])\n",
    "                                        j2 = int(bb_tensor[k][2])\n",
    "                                        output_bw_tensor[idx,0,i1:i2,j1] = 255\n",
    "                                        output_bw_tensor[idx,0,i1:i2,j2] = 255\n",
    "                                        output_bw_tensor[idx,0,i1,j1:j2] = 255\n",
    "                                        output_bw_tensor[idx,0,i2,j1:j2] = 255\n",
    "                                        im_tensor[idx,0,i1:i2,j1] = 255\n",
    "                                        im_tensor[idx,0,i1:i2,j2] = 255\n",
    "                                        im_tensor[idx,0,i1,j1:j2] = 255\n",
    "                                        im_tensor[idx,0,i2,j1:j2] = 255\n",
    "                            display_tensor[:batch_size,:,:,:] = output_bw_tensor\n",
    "                            display_tensor[batch_size:2*batch_size,:,:,:] = im_tensor\n",
    "    \n",
    "                            for batch_im_idx in range(batch_size):\n",
    "                                for mask_layer_idx in range(max_num_objects):\n",
    "                                    for i in range(image_size[0]):\n",
    "                                        for j in range(image_size[1]):\n",
    "                                            if mask_layer_idx == 0:\n",
    "                                                if 25 < outputs[batch_im_idx,mask_layer_idx,i,j] < 85:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 255\n",
    "                                                else:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 50\n",
    "                                            elif mask_layer_idx == 1:\n",
    "                                                if 65 < outputs[batch_im_idx,mask_layer_idx,i,j] < 135:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 255\n",
    "                                                else:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 50\n",
    "                                            elif mask_layer_idx == 2:\n",
    "                                                if 115 < outputs[batch_im_idx,mask_layer_idx,i,j] < 185:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 255\n",
    "                                                else:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 50\n",
    "                                            elif mask_layer_idx == 3:\n",
    "                                                if 165 < outputs[batch_im_idx,mask_layer_idx,i,j] < 230:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 255\n",
    "                                                else:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 50\n",
    "                                            elif mask_layer_idx == 4:\n",
    "                                                if outputs[batch_im_idx,mask_layer_idx,i,j] > 210:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 255\n",
    "                                                else:\n",
    "                                                    outputs[batch_im_idx,mask_layer_idx,i,j] = 50\n",
    "    \n",
    "                                    display_tensor[2*batch_size+batch_size*mask_layer_idx+batch_im_idx,:,:,:]= outputs[batch_im_idx,mask_layer_idx,:,:]\n",
    "                            self.dl_studio.display_tensor_as_image(\n",
    "                               torchvision.utils.make_grid(display_tensor, nrow=batch_size, normalize=True, padding=2, pad_value=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850da764-6879-4ec7-b8cb-91cb986d567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Function to calculate only Dice Loss\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, epsilon=1e-3):\n",
    "    intersection = torch.sum(y_true * y_pred)\n",
    "    union = torch.sum(y_true) + torch.sum(y_pred)\n",
    "    dice_coeff = (2. * intersection + epsilon) / (union + epsilon)\n",
    "    return dice_coeff\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e208f9-9b28-452b-a62b-ea65747206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_dl_2 = DLStudio(\n",
    "#                  dataroot = \"/home/kak/ImageDatasets/PurdueShapes5MultiObject/\",\n",
    "                  dataroot = \"./data/data/\",\n",
    "                  image_size = [64,64],\n",
    "                  path_saved_model = \"./saved_model2\",\n",
    "                  momentum = 0.9,\n",
    "                  learning_rate = 1e-4,\n",
    "                  epochs = 6,\n",
    "                  batch_size = 4,\n",
    "                  classes = ('rectangle','triangle','disk','oval','star'),\n",
    "                  #use_gpu = True,\n",
    "              )\n",
    "max_num_objects_dl_2 = 5\n",
    "segmenter_dl_2 = SemanticSegmentation( dls_dl_2, max_num_objects_dl_2)\n",
    "dataset_file_train = \"PurdueShapes5MultiObject-10000-train.gz\"\n",
    "test_or_train_train = 'train'\n",
    "test_or_train_test = 'test'\n",
    "dataserver_train_2 = SemanticSegmentation.PurdueShapes5MultiObjectDataset(dls_dl_2, segmenter_dl_2, test_or_train_train, dataset_file_train)\n",
    "dataset_file_test = \"PurdueShapes5MultiObject-1000-test.gz\"\n",
    "dataserver_test_2 = SemanticSegmentation.PurdueShapes5MultiObjectDataset(\n",
    "                          dls_dl_2,\n",
    "                          segmenter_dl_2,\n",
    "                          test_or_train_test,\n",
    "                          dataset_file_test,\n",
    "                        )\n",
    "segmenter_dl_2.dataserver_train = dataserver_train_2\n",
    "segmenter_dl_2.dataserver_test = dataserver_test_2\n",
    "\n",
    "segmenter_dl_2.load_PurdueShapes5MultiObject_dataset(dataserver_train_2, dataserver_test_2)\n",
    "\n",
    "model_dl_2 = segmenter_dl_2.mUnet(skip_connections=True, depth=16)\n",
    "#model = segmenter.mUnet(skip_connections=False, depth=4)\n",
    "\n",
    "number_of_learnable_params_dl_2 = sum(p.numel() for p in model_dl_2.parameters() if p.requires_grad)\n",
    "print(\"\\n\\nThe number of learnable parameters in the model: %d\\n\" % number_of_learnable_params_dl_2)\n",
    "\n",
    "num_layers_dl_2 = len(list(model_dl_2.parameters()))\n",
    "print(\"\\nThe number of layers in the model: %d\\n\\n\" % num_layers_dl_2)\n",
    "\n",
    "\n",
    "combined_loss_2 = segmenter_dl_2.run_code_for_training_for_semantic_segmentation_dl(model_dl_2)\n",
    "\n",
    "segmenter_dl_2.save_model(model_dl_2)\n",
    "\n",
    "import pymsgbox\n",
    "response = pymsgbox.confirm(\"Finished training.  Start testing on unseen data?\")\n",
    "if response == \"OK\": \n",
    "    segmenter_dl_2.run_code_for_testing_semantic_segmentation_dl(model_dl_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
